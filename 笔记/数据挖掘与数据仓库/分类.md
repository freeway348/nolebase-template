# 分类

---

### 一、决策树分类
### 数据挖掘的原理

数据挖掘的核心是从数据中提取隐藏的模式和信息，其基本原理包括以下几点：

1. **统计学原理**：基于概率分布、假设检验等方法进行推断和预测。
2. **机器学习原理**：通过数据训练模型，包括监督学习（分类、回归）和无监督学习（聚类、降维）。
3. **搜索与优化**：在搜索空间中寻找最优模式，例如在关联规则挖掘中寻找支持度和置信度最高的规则。
4. **数据库技术**：利用SQL查询、OLAP等工具高效处理和存储数据。

以下是针对决策树、关联挖掘和聚类分析三种方法的具体分析。

---

### 决策树算法的原理

决策树是一种基于树状结构的分类或回归方法，其原理包括以下核心步骤：

1. **特征选择**：
    
    - 目标是选择最优特征分裂数据，使得每次分裂后节点的纯度最大。
    - 常用评价指标：
        - **信息增益**（ID3）：衡量通过特征分裂后，信息不确定性减少的程度。 $\text{信息增益} = H(D) - \sum_{i=1}^k \frac{|D_i|}{|D|} H(D_i)$
        - **信息增益比**（C4.5）：修正信息增益的偏向问题，避免过多选择取值多的特征。 $\text{信息增益比} = \frac{\text{信息增益}}{\text{特征熵}}$
        - **基尼指数**（CART）：评估节点纯度，基尼指数越小，节点越纯。$\text{基尼指数} = 1 - \sum_{i=1}^C p_i^2$
2. **树的构建**：
    
    - 递归分裂数据，生成内部节点和叶子节点。
    - 停止条件：
        - 节点中样本数量低于设定阈值。
        - 当前节点的样本纯度达到要求。
        - 预设树的最大深度。
3. **剪枝（Pruning）**：
    
    - 防止决策树过拟合，通过限制深度或合并叶子节点优化泛化能力。
    - **预剪枝**：在构建过程中提前停止分裂。
    - **后剪枝**：先构建完整树，再通过评估去除无效分枝。

---

### 关联挖掘的原理

关联挖掘的目标是从数据集中发现频繁模式和关联规则。其原理基于支持度和置信度的计算，以及频繁项集的挖掘：

1. **支持度与置信度**：
    
    - **支持度**（Support）：某项集在数据集中出现的频率，用于衡量模式的普遍性。 $\text{支持度} = \frac{\text{项集出现次数}}{\text{交易总数}}$
    - **置信度**（Confidence）：在包含条件项的交易中，同时包含结果项的比例，衡量规则的可靠性。$\text{置信度} = \frac{\text{条件项和结果项同时出现次数}}{\text{条件项出现次数}}$
2. **频繁项集挖掘**：
    
    - 利用支持度筛选频繁项集：
        - **Apriori算法**：基于“频繁项集的所有子集也是频繁项集”这一性质，逐层生成候选项集。
        - **FP-Growth算法**：通过构建频繁模式树避免候选项集生成，提高效率。
3. **规则生成**：
    
    - 从频繁项集中生成关联规则，计算置信度并筛选符合要求的规则。

---

### 聚类分析的原理

聚类分析是一种无监督学习方法，基于数据点之间的相似性进行分组。其原理包括以下步骤：

1. **距离度量**：
    
    - 使用距离或相似性衡量样本之间的关系：
        - 欧氏距离：适用于连续型数据。 $d(x,y)=∑i=1n(xi−yi)2d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$
        - 曼哈顿距离、余弦相似度等。
2. **聚类算法**：
    
    - **K均值**：
        - 随机初始化K个质心。
        - 分配每个样本到最近的质心。
        - 更新质心为簇内样本的均值，重复直到收敛。
    - **层次聚类**：
        - 自底向上：每次合并最近的两个簇。
        - 自顶向下：每次拆分最不相似的簇。
    - **密度聚类（DBSCAN）**：
        - 核心思想是找到高密度区域，并将低密度点归为噪声。
        - 参数包括`ε`（邻域半径）和`MinPts`（最小邻域点数）。
3. **评价指标**：
    
    - **组内平方和（WSS）**：度量样本到簇质心的距离。
    - **轮廓系数**：结合组内紧密性和组间分离性，取值范围为[-1, 1]。

---

### 总结

- **决策树**基于特征选择进行数据分裂，适用于分类和回归任务。
- **关联挖掘**利用频繁项集挖掘和规则生成，发现数据项之间的关联关系。
- **聚类分析**通过相似性度量对无标签数据进行分组，寻找数据的内在结构。


### 二、信息熵
#### 🌟计算公式

$H(X)=-\sum_{i=1}^{n} p(x_i)log_{p(x_i)}$
- $lim_{p\rightarrow\infty}plog_p=0$
- 范围是 $0\leq H(x)\leq logn$

信息熵越大，训练样品集越“不纯”，反之，信息熵越小，训练样品集越“纯”

### 三、ID3算法

![](assets/2afd3517af46ca1b5e58e68fa5e4a16f.jpg)
- 例如：计算`Income`的信息熵，Insurance是我们的评判标准，所以只看这两列
- 首先看`Income`中分为`high`和`low`两种情况：
	- `Income`为`low`时，看其对应的`Insurance`的分布情况，发现这六种情况中，`yes`占2个,`no`占4个，则其信息熵为：$-\frac{1}{3}log\frac{1}{3}-\frac{2}{3}log\frac{2}{3}$
- 信息增益为：Info（T）- Info（ Income）
	- 所以信息增益越大越好
	- Info（T）只看Insurance这一列
- 信息熵越低，纯度也就越高


### 四、Gini

